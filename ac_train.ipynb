{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ac_train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jeo1o58NnIeQ"
      },
      "source": [
        "import numpy as np\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "import pandas as pd\n",
        "import pandas_datareader.data as web\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "global tr_p\n",
        "tr_p =3100\n",
        "global tr_date\n",
        "tr_date = '2015/3/25'\n",
        "global tr_finish\n",
        "tr_finish = False\n",
        "global hidden\n",
        "hidden = (torch.autograd.Variable(torch.zeros(1, 1, 16)),\n",
        "                                         torch.autograd.Variable(torch.zeros(1, 1, 16)))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh5Lr0ZBnnGr"
      },
      "source": [
        "torch.manual_seed(543)\n",
        "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
        "import requests\n",
        "url = \"https://github.com/xiaoa5/stock-technical-analysis/raw/master/000300tech1.csv\"\n",
        "r = requests.get(url)\n",
        "\n",
        "with open('000300tech1.csv','w') as f:\n",
        "  f.write(r.text)\n",
        "df = pd.read_csv('000300tech1.csv', parse_dates=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P64WXp2MoO_F"
      },
      "source": [
        "def load_csv_data(df,col):  \n",
        "    data_pandas = df[col]\n",
        "    data = data_pandas .to_numpy()\n",
        "    result = data\n",
        "    return result\n",
        "def normalise_windows(window_data):\n",
        "    \n",
        "    normalised_window = [((float(p) / float(window_data[-1])) *0.5) for p in window_data]\n",
        "        \n",
        "    return array(normalised_window)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpflo1tYoXLC"
      },
      "source": [
        "class stock_env:\n",
        "    def __init__(self):\n",
        "        print (\"init\")\n",
        "        self.price1 = load_csv_data(df,'close')\n",
        "        self.price2 = load_csv_data(df,'open')\n",
        "        self.dates = df['date']\n",
        "        self.hist = 100\n",
        "        self.train_episode = tr_p\n",
        "        self.test_episode = len(self.price1) - self.hist - 2\n",
        "        self.train_done = False\n",
        "        self.test_done = False\n",
        "        self.budget = 10000\n",
        "        self.num_stocks = 0\n",
        "        self.tr = 25/100000\n",
        "\n",
        "    def reset(self):   \n",
        "        global steps_done\n",
        "        steps_done = 0\n",
        "        self.budget = 10000\n",
        "        self.num_stocks = 0\n",
        "        self.train_done = False\n",
        "        self.test_done = False\n",
        "        \n",
        "        state =normalise_windows( self.price1[steps_done:steps_done+self.hist])\n",
        "        return state\n",
        "\n",
        "    def step(self, action): \n",
        "        global steps_done\n",
        "        steps_done += 1\n",
        "##        if steps_done > self.train_episode:\n",
        "##            self.train_done = True\n",
        "        if steps_done > self.test_episode:\n",
        "            self.test_done = True    \n",
        "\n",
        "        pre_price = float(self.price2[steps_done + self.hist-1+1])\n",
        "        pre_portfolio = self.budget + self.num_stocks * pre_price\n",
        "        pre_date = self.dates[steps_done + self.hist-1+1]\n",
        "        if action == 0 and pre_price >0: #buy\n",
        "                    \n",
        "            while self.budget >= pre_price*(1+self.tr):           \n",
        "                self.budget -= pre_price *(1+self.tr)\n",
        "                self.num_stocks += 1\n",
        "        elif action == 1 and pre_price >0:       #sell\n",
        "           \n",
        "            while self.num_stocks > 0:       \n",
        "                self.budget += pre_price*(1-self.tr)\n",
        "                self.num_stocks -= 1\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "        try:\n",
        "            state = normalise_windows(self.price1[steps_done:steps_done+self.hist])\n",
        "            price = float(self.price2[steps_done + self.hist+1])\n",
        "            portfolio = self.budget + self.num_stocks * price\n",
        "            date = self.dates[steps_done + self.hist+1]\n",
        "            if date == tr_date:\n",
        "                self.train_done = True\n",
        "                reward = np.log2(portfolio /self.budget)*2000\n",
        "##                reward = (portfolio /self.budget - 3.2)*1000\n",
        "            else:\n",
        "                reward = 0\n",
        "                \n",
        "\n",
        "            reward = (200*(portfolio / pre_portfolio)) - (100*((price / pre_price)+1))\n",
        "        except:\n",
        "            reward = 0\n",
        "        return (state,reward,self.train_done,self.test_done,pre_price,pre_portfolio,pre_date)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXxQZ3yqoY_E"
      },
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.rnnLayer = 1\n",
        "        self.rnnIn = 16\n",
        "        self.rnnOut = 16\n",
        "        self.lstm = torch.nn.LSTM(self.rnnIn, self.rnnOut,self.rnnLayer)\n",
        "        self.cnn1 = torch.nn.Conv1d(1,16,9,stride=2, padding=0, dilation=1, groups=1, bias=True)\n",
        "        self.affine1 = nn.Linear(46*16, 128)\n",
        "        self.midd1 = nn.Linear(128, 128)\n",
        "##        self.midd2 = nn.Linear(128, 128)\n",
        "##        self.midd3 = nn.Linear(128, 128)\n",
        "        self.action_head = nn.Linear(128, 2)\n",
        "        self.value_head = nn.Linear(128, 1)\n",
        "\n",
        "        self.saved_actions = []\n",
        "        self.rewards = []\n",
        "        \n",
        "\n",
        "    def forward(self, x,hiddens):\n",
        "        x = x.view(1,1,100)\n",
        "        x = self.cnn1(x)\n",
        "        x = x.view(46,1,16)\n",
        "        x, hiddens = self.lstm(x,hiddens)\n",
        "        x = x.view(-1)\n",
        "        x = F.tanh(self.affine1(x))\n",
        "        x = F.tanh(self.midd1(x))\n",
        "##        x = F.tanh(self.midd2(x))\n",
        "##        x = F.relu(self.midd3(x))\n",
        "        action_scores = self.action_head(x)\n",
        "        state_values = self.value_head(x)\n",
        "        return F.softmax(action_scores, dim=-1), state_values, hiddens\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWtQ9jSJodQS",
        "outputId": "4eee82ee-9723-458a-841d-ab17029a5221"
      },
      "source": [
        "model = Policy()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-3)  #lr=3e-3\n",
        "env = stock_env()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVHlKUDKopyd"
      },
      "source": [
        "def select_action(state,hidden):\n",
        "    state = torch.from_numpy(state).float()\n",
        "    probs, state_value,hidden = model(Variable(state),hidden)\n",
        "##    print('action',probs)\n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "##    print('Categorical',action)\n",
        "\n",
        "    if tr_finish == False:\n",
        "    \n",
        "        model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
        "##    print(action.data[0])\n",
        "    return action.data\n",
        "\n",
        "\n",
        "def finish_episode():\n",
        "    R = 0\n",
        "    saved_actions = model.saved_actions\n",
        "    policy_losses = []\n",
        "    value_losses = []\n",
        "    rewards = []\n",
        "\n",
        "    for r in model.rewards[::-1]:\n",
        "##        print('r',r)\n",
        "##        print('r-----------------------',r)\n",
        "        R = r + 0.999 * R\n",
        "        rewards.insert(0, R)\n",
        "    rewards = torch.Tensor(rewards)\n",
        "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
        "  \n",
        "    for (log_prob, value), r in zip(saved_actions, rewards):\n",
        "        reward = r - value.data\n",
        "\n",
        "        policy_losses.append(-log_prob * reward)\n",
        "        value_losses.append(F.smooth_l1_loss(value, Variable(torch.Tensor([r]))))\n",
        "    optimizer.zero_grad()\n",
        "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    del model.rewards[:]\n",
        "    del model.saved_actions[:]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ6-lpXLpLOZ",
        "outputId": "a54b114e-1032-463b-d556-370359e4674c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7hnu5Tvo2mV"
      },
      "source": [
        "def main():\n",
        "    model_save_name = \"stock_ac2.pkl\"\n",
        "    save_path= F\"/content/drive/My Drive/Colab Notebooks/files/{model_save_name}\" \n",
        "    running_rewards = []\n",
        "    results = []\n",
        "    profits = []\n",
        "    train_results = []\n",
        "    periods = []\n",
        "    benchresults = []\n",
        "    dates = []\n",
        "    colors = []\n",
        "    profit_max = 0\n",
        "    for i_episode in count(1):\n",
        "        state = env.reset()\n",
        "        hidden = (torch.autograd.Variable(torch.zeros(1, 1, 16)),\n",
        "                                         torch.autograd.Variable(torch.zeros(1, 1, 16)))\n",
        "##        state = torch.from_numpy(state)\n",
        "##        state = state.unsqueeze(0)\n",
        "        for t in range(10000):  # Don't infinite loop while learning\n",
        "            action = select_action(state,hidden)\n",
        "            \n",
        "            next_state, reward, tr_finish, ts_done,price,portfolio,date = env.step(action)\n",
        "##            next_state = torch.from_numpy(next_state)\n",
        "##            next_state = next_state.unsqueeze(0)\n",
        "\n",
        "            if t == 0:\n",
        "                zero_portfolio = portfolio\n",
        "                zero_value = price\n",
        "##            if t == 3101:\n",
        "##                ref_portfolio = portfolio\n",
        "##                ref_value = price\n",
        "\n",
        "            if date == tr_date:\n",
        "                ref_portfolio = portfolio\n",
        "                ref_value = price\n",
        "##                record = True\n",
        "            \n",
        "            if tr_finish == False:\n",
        "                model.rewards.append(reward)\n",
        "            if ts_done:\n",
        "                break\n",
        "            state = next_state\n",
        "        #----------------------------------\n",
        "\n",
        "        periods.append(i_episode)\n",
        "        \n",
        "        results.append(portfolio)\n",
        "        train_results.append(ref_portfolio/zero_portfolio)\n",
        "        profit = portfolio/ref_portfolio\n",
        "        profits.append(profit)\n",
        "        benchmark = price/ref_value\n",
        "        print('episode {:4d}'.format(i_episode),\n",
        "                  'portfolio {:9.2f}'.format(ref_portfolio),   \n",
        "                  'benchmark {:5.3f}'.format(benchmark),\n",
        "                  'profit {:5.3f}'.format(profit))\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "        try:\n",
        "            temp = running_reward\n",
        "        except:\n",
        "            running_reward = portfolio\n",
        "            \n",
        "        running_reward = running_reward * 0.95 + portfolio * 0.05\n",
        "        running_rewards.append(running_reward)\n",
        "        \n",
        "        finish_episode()\n",
        "        if profit > profit_max and ref_portfolio > 70000 :\n",
        "            \n",
        "            profit_max = profit\n",
        "            torch.save(model, save_path)\n",
        "            print('model saved.')\n",
        "        if i_episode % 10 == 0:\n",
        "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
        "                i_episode, t, running_reward))\n",
        "        if running_reward > 100000: #env.spec.reward_threshold:\n",
        "            print(\"Solved! Running reward is now {} and \"\n",
        "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    fig, ax1 = plt.subplots()\n",
        "\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Train episodes')\n",
        "    ax1.set_ylabel('Train Profit (bis 25 März 2015)', color=color)\n",
        "    ax1.plot(periods,train_results, color=color)\n",
        "    ax1.tick_params(axis='y', labelcolor=color)    \n",
        "\n",
        "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Validation Profit (seit 26 März 2015)', color=color)  # we already handled the x-label with ax1\n",
        "    ax2.plot(periods,profits, color=color)\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "    plt.show()\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9PcQDvNp2Qg",
        "outputId": "a8bc9499-bd6d-4e9a-d4cb-29d57fab145a"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode    1 portfolio  12930.55 benchmark 1.014 profit 1.034\n",
            "episode    2 portfolio  11709.42 benchmark 1.014 profit 0.751\n",
            "episode    3 portfolio  21159.06 benchmark 1.014 profit 1.151\n",
            "episode    4 portfolio  10917.34 benchmark 1.014 profit 0.599\n",
            "episode    5 portfolio  10012.66 benchmark 1.014 profit 1.316\n",
            "episode    6 portfolio  20791.72 benchmark 1.014 profit 0.908\n",
            "episode    7 portfolio  16945.42 benchmark 1.014 profit 1.081\n",
            "episode    8 portfolio  17402.91 benchmark 1.014 profit 0.817\n",
            "episode    9 portfolio  44983.94 benchmark 1.014 profit 1.323\n",
            "episode   10 portfolio  10282.04 benchmark 1.014 profit 0.903\n",
            "Episode 10\tLast length:  3768\tAverage length: 15808.69\n",
            "episode   11 portfolio  17882.41 benchmark 1.014 profit 1.075\n",
            "episode   12 portfolio   5574.60 benchmark 1.014 profit 1.152\n",
            "episode   13 portfolio  11455.24 benchmark 1.014 profit 0.970\n",
            "episode   14 portfolio  11987.79 benchmark 1.014 profit 0.936\n",
            "episode   15 portfolio  19371.81 benchmark 1.014 profit 0.830\n",
            "episode   16 portfolio  13268.26 benchmark 1.014 profit 0.673\n",
            "episode   17 portfolio   8121.68 benchmark 1.014 profit 0.720\n",
            "episode   18 portfolio  20562.33 benchmark 1.014 profit 0.722\n",
            "episode   19 portfolio  15730.48 benchmark 1.014 profit 1.296\n",
            "episode   20 portfolio  18540.51 benchmark 1.014 profit 1.036\n",
            "Episode 20\tLast length:  3768\tAverage length: 14920.80\n",
            "episode   21 portfolio  17788.81 benchmark 1.014 profit 0.937\n",
            "episode   22 portfolio  13993.70 benchmark 1.014 profit 1.299\n",
            "episode   23 portfolio  18064.63 benchmark 1.014 profit 0.888\n",
            "episode   24 portfolio   8578.07 benchmark 1.014 profit 0.743\n",
            "episode   25 portfolio   9557.17 benchmark 1.014 profit 0.718\n",
            "episode   26 portfolio  29337.65 benchmark 1.014 profit 0.565\n",
            "episode   27 portfolio  14148.91 benchmark 1.014 profit 1.159\n",
            "episode   28 portfolio  22028.13 benchmark 1.014 profit 0.877\n",
            "episode   29 portfolio  23041.91 benchmark 1.014 profit 0.941\n",
            "episode   30 portfolio  20063.42 benchmark 1.014 profit 0.991\n",
            "Episode 30\tLast length:  3768\tAverage length: 15395.52\n",
            "episode   31 portfolio  18342.39 benchmark 1.014 profit 1.227\n",
            "episode   32 portfolio  12146.39 benchmark 1.014 profit 0.886\n",
            "episode   33 portfolio   4767.14 benchmark 1.014 profit 0.714\n",
            "episode   34 portfolio  12603.96 benchmark 1.014 profit 1.165\n",
            "episode   35 portfolio  26499.03 benchmark 1.014 profit 0.777\n",
            "episode   36 portfolio  23458.49 benchmark 1.014 profit 0.611\n",
            "episode   37 portfolio  13806.92 benchmark 1.014 profit 1.271\n",
            "episode   38 portfolio   6584.26 benchmark 1.014 profit 1.251\n",
            "episode   39 portfolio   8054.80 benchmark 1.014 profit 0.951\n",
            "episode   40 portfolio  18402.62 benchmark 1.014 profit 1.061\n",
            "Episode 40\tLast length:  3768\tAverage length: 14787.04\n",
            "episode   41 portfolio   9606.60 benchmark 1.014 profit 1.038\n",
            "episode   42 portfolio  66858.33 benchmark 1.014 profit 0.998\n",
            "episode   43 portfolio  14297.07 benchmark 1.014 profit 0.727\n",
            "episode   44 portfolio  15674.50 benchmark 1.014 profit 0.635\n",
            "episode   45 portfolio  29069.72 benchmark 1.014 profit 0.820\n",
            "episode   46 portfolio   6226.38 benchmark 1.014 profit 0.973\n",
            "episode   47 portfolio  14874.59 benchmark 1.014 profit 0.813\n",
            "episode   48 portfolio  21810.48 benchmark 1.014 profit 1.261\n",
            "episode   49 portfolio  11302.11 benchmark 1.014 profit 0.980\n",
            "episode   50 portfolio  21899.23 benchmark 1.014 profit 1.142\n",
            "Episode 50\tLast length:  3768\tAverage length: 16814.89\n",
            "episode   51 portfolio  17770.90 benchmark 1.014 profit 1.212\n",
            "episode   52 portfolio  32417.34 benchmark 1.014 profit 0.910\n",
            "episode   53 portfolio  13713.63 benchmark 1.014 profit 1.815\n",
            "episode   54 portfolio  13474.01 benchmark 1.014 profit 0.836\n",
            "episode   55 portfolio  41285.70 benchmark 1.014 profit 0.912\n",
            "episode   56 portfolio   9098.56 benchmark 1.014 profit 0.773\n",
            "episode   57 portfolio   4436.39 benchmark 1.014 profit 0.945\n",
            "episode   58 portfolio  24001.41 benchmark 1.014 profit 0.660\n",
            "episode   59 portfolio  22171.63 benchmark 1.014 profit 0.913\n",
            "episode   60 portfolio  20747.83 benchmark 1.014 profit 0.956\n",
            "Episode 60\tLast length:  3768\tAverage length: 17599.24\n",
            "episode   61 portfolio  36005.66 benchmark 1.014 profit 0.804\n",
            "episode   62 portfolio  37968.57 benchmark 1.014 profit 0.645\n",
            "episode   63 portfolio  20502.41 benchmark 1.014 profit 0.799\n",
            "episode   64 portfolio  26194.30 benchmark 1.014 profit 0.920\n",
            "episode   65 portfolio  11711.57 benchmark 1.014 profit 0.801\n",
            "episode   66 portfolio  30211.93 benchmark 1.014 profit 1.041\n",
            "episode   67 portfolio  12159.71 benchmark 1.014 profit 1.177\n",
            "episode   68 portfolio  37470.31 benchmark 1.014 profit 0.822\n",
            "episode   69 portfolio  10717.51 benchmark 1.014 profit 0.830\n",
            "episode   70 portfolio  12129.92 benchmark 1.014 profit 1.047\n",
            "Episode 70\tLast length:  3768\tAverage length: 18425.10\n",
            "episode   71 portfolio  12572.27 benchmark 1.014 profit 1.222\n",
            "episode   72 portfolio  15977.99 benchmark 1.014 profit 0.916\n",
            "episode   73 portfolio  14133.43 benchmark 1.014 profit 0.905\n",
            "episode   74 portfolio   7440.10 benchmark 1.014 profit 0.900\n",
            "episode   75 portfolio   9614.73 benchmark 1.014 profit 0.843\n",
            "episode   76 portfolio  28155.79 benchmark 1.014 profit 0.869\n",
            "episode   77 portfolio  24162.07 benchmark 1.014 profit 1.008\n",
            "episode   78 portfolio  13267.92 benchmark 1.014 profit 0.719\n",
            "episode   79 portfolio  19050.99 benchmark 1.014 profit 0.820\n",
            "episode   80 portfolio  12759.14 benchmark 1.014 profit 1.258\n",
            "Episode 80\tLast length:  3768\tAverage length: 17024.23\n",
            "episode   81 portfolio   8543.73 benchmark 1.014 profit 0.754\n",
            "episode   82 portfolio  11960.68 benchmark 1.014 profit 1.226\n",
            "episode   83 portfolio  10730.82 benchmark 1.014 profit 0.812\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}